---
title: "GroupAssignment"
output: html_document
date: "2023-10-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup}
data_dir <- "C:/Users/Blake Zimbardi/Desktop/r/working/Train Data"
source_dir <-"C:/Users/Blake Zimbardi/Desktop/r/source"

library(magrittr)
library(ggplot2)
library(psych)
library(tidyverse)
library(lattice)
library(dplyr)
library(here)
library(ggpubr)
library(ggfortify)
library(MASS)
library(lindia)
library(olsrr)
library(car)

setwd(source_dir)
getwd()
source("AccidentInput.R")
source("PCAplots.R")
list = file.inputl(data_dir)
df = combine.data(list, )

# Add labels to TYPEQ
df$TYPE <- factor(df$TYPE, labels = c("Derailment", "HeadOn", 
                                                "Rearend", "Side", "Raking", "BrokenTrain", "Hwy-Rail", 
                                                "GradeX", "Obstruction", "Explosive", "Fire","Other",
                                                "SeeNarrative" ))

df$TYPEQ <- factor(df$TYPEQ, labels = c("NA", "NA", "Freight", "Passenger", "Commuter", 
                                                  "Work",  "Single", "CutofCars", "Yard", "Light", "Maint",
                                                  "MaintOfWay", "Passenger", "Commuter", "ElectricMulti", "ElectricMulti"))
df$Cause <- rep(NA, nrow(df))
df$Cause[which(substr(df$CAUSE, 1, 1) == "M")] <- "M"
df$Cause[which(substr(df$CAUSE, 1, 1) == "T")] <- "T"
df$Cause[which(substr(df$CAUSE, 1, 1) == "S")] <- "S"
df$Cause[which(substr(df$CAUSE, 1, 1) == "H")] <- "H"
df$Cause[which(substr(df$CAUSE, 1, 1) == "E")] <- "E"
df$Cause <- factor(df$Cause)
```

### {Hypothesis (1)}
Null Hypothesis: The type of train and type of accident in a railroad accident does not have a statistically significant relationship with casualties. 
Alternative Hypothesis: The type of train and type of accident in a railroad accident have a statistically significant relationship with casualties. 

These hypothesis are actionable since testing this hypothesis will reveal the type of train and type of accidents that are involved with significant impacts on casualties in train accidents. Train safety can be improved by further analyzing accidents involving the most statistically significant type of trains in a train accident to determine what safety components of the train design and operation (i.e. conductor visibility, braking mechanisms, training, regulations) could be changed in favor of reducing severity of tail accidents.

For preliminary investigation of these hypothesis, we generate box plots comparing total number of casualties vs. different types of trains and vs. different types of accidents. Similarly, we observe that the type of train with largest number of accidents involving 1 or more casualties are freight trains. Finally, the East Palestine train derailment on February 3rd, 2023 peaked our interest in Freight Train accidents. While no casualties were reported, other notable freight train accidents like the 1986 Miamisburg train derailment (also in Ohio) and the 2005 Graniteville train crash (in South Carolina) are similar freight train accidents that not only caused multiple casualties, but also released toxic chemicals to the environment.

East Palestine: https://www.theguardian.com/us-news/2023/feb/11/ohio-train-derailment-wake-up-call
Miamisburg: https://www.washingtonpost.com/archive/politics/1986/07/09/17000-evacuated-after-derailment/f6a0f635-bebb-4342-8c62-23304b12876e/
Graniteville: https://web.archive.org/web/20140719212353/http://www.wjbf.com/story/21686984/federal-prosecutors-say-norfolk-southern-should-be-fined-for-graniteville-pollution 
###

### Treatment of variables 3(b)
In order to conduct preliminary investigation, we must first cleanup the train data set. This involves creating a new "Causality" variable with total killed (TOTKLD) and total injured (TOTINJ) together.  We remove the data with no casualties from our data set as well as null, empty, and duplicated data. Finally, a severe outlier was removed in order to prevent high leveraged points appearing in our model.
###

```{r create casualty, data cleanup, and boxplots}
# Create Casualty variable
df["Causalty"] = df["TOTKLD"] + df["TOTINJ"]
# Remove data without a Casualty
totacts_posCas <- filter(df, Causalty > 0)
# Remove data with Null or empty
totacts_posCas_null <- filter(totacts_posCas, TYPEQ != "NULL" & TYPEQ != "")
# Remove duplicate reports
totacts_posCas_nd <- totacts_posCas_null %>% distinct(INCDTNO, YEAR, MONTH, DAY, TIMEHR, TIMEMIN, .keep_all = TRUE)
# Remove outlier
totacts_posCas_nd <- totacts_posCas_nd[-c(which.max(totacts_posCas_nd$Causalty)), ]

# Box Plots of Type of Train vs. Total Casualties per accident
ggplot(data = totacts_posCas_nd, aes(x = TYPEQ, y = Causalty)) +
  geom_boxplot() +
  coord_flip() +
  scale_fill_grey(start = 0.5, end = 0.8) +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggtitle("Box Plots of Total Causalties") +
  labs(y = "# of Causalties", x = "Type of Train")
# Box Plots of Type of Accident vs. Total Casualties per accident
ggplot(data = totacts_posCas_nd, aes(x = TYPE, y = Causalty)) +
  geom_boxplot() +
  coord_flip() +
  scale_fill_grey(start = 0.5, end = 0.8) +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggtitle("Box Plots of Total Causalties") +
  labs(y = "# of Causalties", x = "Type of Accident")

# Type of train and accident w/ largest number of accidents w/ >=1 casualties
table(totacts_posCas_nd$TYPEQ)
table(totacts_posCas_nd$TYPE)

# Reset row names
rownames(totacts_posCas_nd) <- NULL
```

### Interpretation 3(a), 3(b)
Based on the results from the box plots and data tables, we observe that most of the data groups around the left side of the box plots, demonstrating that the majority of severe accidents occur regardless of the type of accident of type of train involved. Despite this, we observe a higher frequency of large number of casualties for Derailment accidents and Freight and Passenger train accidents. It's thus implied there may be an interaction between freight and/or passenger trains and derailment accidents, so we next build a full main effects model with interactions between derailments, freight trains, passenger trains, and casualties. 

To accomplish this, we must create three new binary columns: Freight, Passenger, and Derailment, where:
Freight = whether the accident has a Freight train involved (1 if yes, 0 if no)
Passenger = whether the accident has a Passenger train involved (1 if yes, 0 if no)
Derailment = whether the accident is due to the derailment of the train (1 if yes, 0 if no)

Transforming these predictor variables was necessary as all these variables were categorical with different levels to them. Afterwards, the next step is to build a full main effects model with interactions between derailments and passenger trains and derailments and freight trains.
###

```{r Code predictor variables 2(b)}
# Freight
totacts_posCas_nd$Freight <- 0

for (i in 1:nrow(totacts_posCas_nd)) {
  totacts_posCas_nd[i, "Freight"] <- totacts_posCas_nd$TYPEQ[i] == "Freight" 
}

# Passenger
totacts_posCas_nd$Passenger <- 0

for (i in 1:nrow(totacts_posCas_nd)) {
  # Hwy-rail crossing or RR Grade Crossing
  totacts_posCas_nd[i, "Passenger"] <- totacts_posCas_nd$TYPEQ[i] == "Passenger" 
}

# Derailment
totacts_posCas_nd$Derail <- 0

for (i in 1:nrow(totacts_posCas_nd)) {
  totacts_posCas_nd[i, "Derail"] <- totacts_posCas_nd$TYPE[i] == "Derailment" # Derailment
}
```

```{r 1st model}
causdmg.lm1<-lm(Causalty ~ Derail + Passenger + Freight + Derail*Passenger + Derail*Freight,data=totacts_posCas_nd)
summary(causdmg.lm1)
AIC(causdmg.lm1)
```
### Interpretation 3(c), 3(d), 3(e)
Above is the summary of the main effects model for our hypothesis. The intercept is equal to 2.5083, meaning that when a train accident is not caused by derailment and does not involve a freight train or passenger train, the number of casualties is 2.5083. Derailments, Passenger, the interaction between derailments and passenger, and the interaction between derailment and freight have positive coefficients, meaning that casualties increase if the binary variables are true. Freight trains have a negative relationship with casualties, meaning that casualties decrease if this variable is true. Only the Passenger variable and interaction between derailment and passenger trains are significant at the p < 0.001 level, and the model is overall significant at this level as well.

However, the biggest downside to our main effects model is that it only has an adjusted R-squared value of 0.026 and a high AIC of 30748.47. Since a high adjusted R-squared and low AIC indicates a model is a good fit for the data, this means that this linear model explains very little of the variance of casualties. We investigate alternative models since the R-squared value and AIC are so poor. We decided to adjust our model by building a second order model that includes all pairwise interaction terms to see if the model improves at all.
###

```{r 2nd model}
causdmg.lm2 <- lm(Causalty ~ (Derail + Passenger + Freight + Derail*Passenger + Derail*Freight)^2, 
                   data= totacts_posCas_nd)
summary(causdmg.lm2)
AIC(causdmg.lm2)
```

### Interpretation 3(d), 3(e)
Above is the summary of the second model with all pairwise interaction terms. The results of this model (intercept value, significant variables, coefficients) are identical to the above model, demonstrating that there is no improvement by including all pairwise interaction terms. We attempt to remedy this by utilizing a stepwise regression to determine what the best predictors would be. 
###

```{r 3rd model}
causdmg.lm2.step <- step(causdmg.lm2, trace= F)
summary(causdmg.lm2.step)
AIC(causdmg.lm2.step)
```

### 3(b), 3(c), 3(d), 3(e)
We see that there is minimal improvement in this model as the adjusted R-squared value is now 0.02626, the AIC is 30746.56, and the overall model is still significant at the same level. Interestingly, while Passenger and the interaction between derailment and passenger are still significant at the p < 0.001 level, the derailment variable is now significant at the p < 0.01 level. Also of note is the interaction term between Derail and Freight is removed from the model. 

In another attempt to create an improved model, we add additional binary variables from different levels of the TYPE and TYPEQ variables to further explain accident causualties. Looking back at the box plots, we see that adding Commuter and Electric Multicar trains as well as Obstruction, Rear End, and Head On accidents to the dataset could improve the model. We model these predictor variables as follows:

Commuter = whether the accident has a Commuter train involved (1 if yes, 0 if no)
ElectricMulti = whether the accident has a Electric Multicar train involved (1 if yes, 0 if no)
Obstruction = whether the accident is due to the obstruction on the track (1 if yes, 0 if no)
Rearend = whether the accident is due to the train rear ending something on the track (1 if yes, 0 if no)
HeadOn = whether the accident is due to the train colliding head on with something on the track (1 if yes, 0 if no)

After coding these categorical variables, we create a new main effects model with full interactions between each type of train and type of collision for our created binary variables.
###

``` {r Adding more predictor variables}
# Commuter
totacts_posCas_nd$Commuter <- 0

for (i in 1:nrow(totacts_posCas_nd)) {
  totacts_posCas_nd[i, "Commuter"] <- totacts_posCas_nd$TYPEQ[i] == "Commuter" 
}

# ElectricMulti
totacts_posCas_nd$ElectricMulti <- 0

for (i in 1:nrow(totacts_posCas_nd)) {
  # Hwy-rail crossing or RR Grade Crossing
  totacts_posCas_nd[i, "ElectricMulti"] <- totacts_posCas_nd$TYPEQ[i] == "ElectricMulti" 
}

# Obstruction
totacts_posCas_nd$Obstruction <- 0

for (i in 1:nrow(totacts_posCas_nd)) {
  totacts_posCas_nd[i, "Obstruction"] <- totacts_posCas_nd$TYPE[i] == "Obstruction" # Derailment
}

# Rearend
totacts_posCas_nd$Derail <- 0

for (i in 1:nrow(totacts_posCas_nd)) {
  totacts_posCas_nd[i, "Rearend"] <- totacts_posCas_nd$TYPE[i] == "Rearend" # Derailment
}

# HeadOn
totacts_posCas_nd$Derail <- 0

for (i in 1:nrow(totacts_posCas_nd)) {
  totacts_posCas_nd[i, "HeadOn"] <- totacts_posCas_nd$TYPE[i] == "HeadOn" # Derailment
}
```

``` {r creating a new model}
causdmg.lm3 <- lm(Causalty ~ Derail + Passenger + Freight + Commuter + ElectricMulti + Obstruction + Rearend + HeadOn +  Derail*Passenger + Derail*Freight + Derail*Commuter + Derail*ElectricMulti + Obstruction*Passenger + Obstruction*Freight + Obstruction*Commuter + Obstruction*ElectricMulti + Rearend*Passenger + Rearend*Freight + Rearend*Commuter + Rearend*ElectricMulti + HeadOn*Passenger + HeadOn*Freight + HeadOn*Commuter + HeadOn*ElectricMulti,
                   data= totacts_posCas_nd)
summary(causdmg.lm3)
AIC(causdmg.lm3)
```

### 3(c), 3(e)
We observe that we additional predictor variables involved, the intercept changes to 1.35203 (so approximately 1.35203 casualties per accident), a decrease from our previous models. Passenger, the interaction between Passenger and Rearend, the interaction between passenger and head on, and the interaction between passenger and commuter as significant at the p < 0.001 level. The Commuter, interaction between Commuter and Obstruction, and interaction between ElectricMulti and Obstruction are significant at the p < 0.01 level. There is significance for ElectricMulti at the p < 0.1 level, and overall our model is significant at the p < 0.05 level.

This model's adjusted R-squared value is vastly improved to 0.04098, albeit still incredibly small overall, as well as the AIC to 30702.03. We again run a backwards stepwise regression to see which predictors are best.
###

``` {r}
causdmg.lm3.step <- step(causdmg.lm3, trace= F)
summary(causdmg.lm3.step)
AIC(causdmg.lm3.step)
```

### 3(c)
There is marginal improvement in the adjusted R-squared value to 0.04256, the AIC to 30687.64, and intercept casualty predictor to 1.88299. However, the model is overall now significant at p < 0.001 level, along with the Passenger, interaction between Passenger and Rearend, interaction between passenger and Headon, and interaction between Commuter and Headon. Similarly, the Commuter and the interaction between Commuter and Obstruction and interaction between ElectricMulti and Obstruction are significant at the p < 0.01 level. Like the previous stepwise regression model, the interaction term between Derailment and Freight is removed, along with the Derail, Freight, all interaction terms with Derail and Freight, the interaction term between Commuter and Rearend, the interaction term between ElectricMulti and Rearend, and the interaction term between ElectricMulti and Headon are removed as predictors.

Despite improvements, we must check the diagnostic plots to verify linear regression assumptions are satisfied and if any transformations are required.
###

```{r diagnostic plots}
autoplot(causdmg.lm3.step,which=1, ncol = 1, label.size = 3) + theme_bw() #Residual vs. Fitted

autoplot(causdmg.lm3.step,which=2, ncol = 1, label.size = 3) + theme_bw() #QQ

autoplot(causdmg.lm3.step, which=4, ncol = 1, label.size = 3) + theme_bw() #Cook's distance
```
### 3(c), 3(d), 3(e)
The residuals vs. fitted plot tests for a constant mean of 0 and constant variance. The lack of a constant mean of 0 is expected here since the chosen model is a poor fit for the data with a low adjusted R-squared value. The variance is also heteroscedastic. 

The QQ plot is used to look for close alignment with the quantiles of a normal distribution. Here we can see that the QQ plot violates the assumption and displays heavy-tailed behavior for both ends, but especially for the right tail, going to standard residuals upwards of 30. The QQ plot combined with the heteroscedastic variance indicates that the response variable should be transformed.

Cook's distance is used to identify influential points in the dataset for our model. Here we can see that there are six points above 0.5, making them notably influential and indicating to us that we should remove them.

After analyzing the diagnostics plots, we identified that we need to transform the response variable and remove six influential points. We will remove the six influential points and then create a boxcox plot to see how to transform the casualty data.
###

```{r remove cook's distance points, boxcox plot}
totacts_posCas_nd.rm <- totacts_posCas_nd[-c(1440, 1551, 2417, 2789, 2928, 3211), ]
boxcox(causdmg.lm3.step)
```

### 3(d), 3(e)
The bracket locations are located between -2 and -1. Because of this, a boxcox transformation with the optimal lambda value is necessary. 
###

```{r optimal lambda and new stepwise regression modle}
# Optimal lambda
xval <- which.max(boxcox(causdmg.lm3.step, plotit = F)$y)
lam <- boxcox(causdmg.lm3.step, plotit = F)$x[xval] 

# Run the new interaction regression model with the transformation
causdmg.lm3.boxcox <- lm((Causalty^lam-1)/lam ~ (Derail + Passenger + Freight + Commuter + ElectricMulti + Obstruction + Rearend + HeadOn +  Derail*Passenger + Derail*Freight + Derail*Commuter + Derail*ElectricMulti + Obstruction*Passenger + Obstruction*Freight + Obstruction*Commuter + Obstruction*ElectricMulti + Rearend*Passenger + Rearend*Freight + Rearend*Commuter + Rearend*ElectricMulti + HeadOn*Passenger + HeadOn*Freight + HeadOn*Commuter + HeadOn*ElectricMulti)^2, 
                          data= totacts_posCas_nd.rm)

# Run a stepwise regression with the new interaction model
causdmg.lm3.boxcox.step <- step(causdmg.lm3.boxcox, trace= F)
summary(causdmg.lm3.boxcox.step)
AIC(causdmg.lm3.boxcox.step)
```

### 3(c)
Above is the final stepwise interaction model after removing influential points and performing a boxcox transformation. Unlike the previous stepwise regression model, the Freight Term and interaction between Freight and Rearend are included, and the interaction term between Commuter and Rearend are included in the model. Similarly, the interaction term between Passengers and Headon, the interaction term between Commuter and Headon, the interaction term between ElectricMulti and Obstruction are now absent from this model. Six of the eleven coefficients are significant at the p < 0.001 level, one term is significant at the p < 0.01 level, and one term is significant at the p < 0.05 level, showing greater overall significance among the predictors compared to the previous model. Additionally, looking at the adjusted R-squared value we can see that we have obtained our highest value yet at 0.08292, almost double than the previous model. We also have a much lower AIC of -223.8163, orders of magnitude lower than any of the previous models. Finally, we have an f-statistic with a p-value < 0.001, indicating that the model is significant.
###

``` {r}
autoplot(causdmg.lm3.boxcox.step,which=1, ncol = 1, label.size = 3) + theme_bw() #Residual vs. Fitted

autoplot(causdmg.lm3.boxcox.step,which=2, ncol = 1, label.size = 3) + theme_bw() #QQ

autoplot(causdmg.lm3.boxcox.step, which=4, ncol = 1, label.size = 3) + theme_bw() #Cook's distance
```

### 3(c)
We can also take another look at the diagnostics plots above. After performing a boxcox transformation and removing the six influential points, we can see that our diagnostics plots show significant improvements than before the adjustments. The residual vs. fitted plot has a more constant mean of 0 and more homoscedasticity than the previous model. The QQ plot displays more linear behavior than the previous model, and the Cook's distance values are all below 0.005.

#### Conclusion 4(a)

While our final model improved a lot compared to previous iterations (such as having a great AIC value), there are certainly still problems present with the model. The first is that the adjusted R-squared value, while improved, is still quite low. Despite the low f-statistic p-value indicating that the model is statistically significant, the low adjusted R-squared value makes us very skeptical that this model has much predictive or inferential power. Additionally, while the variance improved from the adjustments we made, it still displays some heteroscedastic behavior and therefore still violates the constant variance assumption. Violating this assumption also makes us skeptical of the model's power since lack of constant variance affects the precision of the model. In addition, heteroscedasticity tends to produce p-values that are deceptively significant, which may be misleading us to think that the model and its coefficients are more significant than they actually are.

Based on our findings above, we fail to reject the null hypothesis. Despite the model producing a significant f-statistic p-value (p-value < 0.0000000000000002), our violation of the constant variance assumption leads us to believe our model is not reliable enough to reject the null hypothesis. And despite having a low AIC of -223.8163, our low adjusted R-squared value of 0.08292 also supports this conclusion.

#### Recommendations 4(b)

The purpose of this hypothesis was to study the relationship between common train accident causes by type of train and type of accident and the amount of casualties with the intention of providing the FRA with recommendations to improve railroad safety. Based on our findings, we have found the type of train and type of accident in a railroad accident does not have a statistically significant relationship with casualties. Some of these factors such as Passenger trains and Commuter trains showed potential to influence the number of casualties more, but we could not gather enough evidence to definitively support that claim. Perhaps the common train accident causes studied in this project result in more no casualty accidents, but that relationship was not studied due to our filtering for train accidents with at least one casualty reported.

We recommend to investigate alternative factors to improve railroad safety as per the FRA's primary goal, but we also believe some of these factors should still be monitored for their potential significance in the future.
